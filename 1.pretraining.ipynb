{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d1e7542-3920-4717-8381-59e997ab2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm \n",
    "from rdkit.Chem import QED\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import itertools\n",
    "import importlib\n",
    "import random\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb812322-b4d3-4aa4-91e1-292c34f301cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokens: ['[=Branch1]', '[#Branch1]', '[=Branch2]', '[#Branch2]', '[Branch1]', '[Branch2]', '[=Ring1]', '[=Ring2]', '[Ring1]', '[Ring2]', '[NH1+1]', '[CH1-1]', '[=N+1]', '[=N-1]', '[=S+1]', '[=PH1]', '[#N+1]', '[N+1]', '[O-1]', '[NH1]', '[CH0]', '[N-1]', '[OH0]', '[PH1]', '[C-1]', '[S+1]', '[CH1]', '[NH0]', '[PH0]', '[SH1]', '[=C]', '[=O]', '[=N]', '[Cl]', '[#C]', '[Br]', '[=S]', '[=P]', '[#N]', '[C]', '[N]', '[O]', '[S]', '[P]', '[F]']\n",
      "word2index: {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3, '[=Branch1]': 4, '[#Branch1]': 5, '[=Branch2]': 6, '[#Branch2]': 7, '[Branch1]': 8, '[Branch2]': 9, '[=Ring1]': 10, '[=Ring2]': 11, '[Ring1]': 12, '[Ring2]': 13, '[NH1+1]': 14, '[CH1-1]': 15, '[=N+1]': 16, '[=N-1]': 17, '[=S+1]': 18, '[=PH1]': 19, '[#N+1]': 20, '[N+1]': 21, '[O-1]': 22, '[NH1]': 23, '[CH0]': 24, '[N-1]': 25, '[OH0]': 26, '[PH1]': 27, '[C-1]': 28, '[S+1]': 29, '[CH1]': 30, '[NH0]': 31, '[PH0]': 32, '[SH1]': 33, '[=C]': 34, '[=O]': 35, '[=N]': 36, '[Cl]': 37, '[#C]': 38, '[Br]': 39, '[=S]': 40, '[=P]': 41, '[#N]': 42, '[C]': 43, '[N]': 44, '[O]': 45, '[S]': 46, '[P]': 47, '[F]': 48}\n",
      "index2word: {0: '<pad>', 1: '<unk>', 2: '<sos>', 3: '<eos>', 4: '[=Branch1]', 5: '[#Branch1]', 6: '[=Branch2]', 7: '[#Branch2]', 8: '[Branch1]', 9: '[Branch2]', 10: '[=Ring1]', 11: '[=Ring2]', 12: '[Ring1]', 13: '[Ring2]', 14: '[NH1+1]', 15: '[CH1-1]', 16: '[=N+1]', 17: '[=N-1]', 18: '[=S+1]', 19: '[=PH1]', 20: '[#N+1]', 21: '[N+1]', 22: '[O-1]', 23: '[NH1]', 24: '[CH0]', 25: '[N-1]', 26: '[OH0]', 27: '[PH1]', 28: '[C-1]', 29: '[S+1]', 30: '[CH1]', 31: '[NH0]', 32: '[PH0]', 33: '[SH1]', 34: '[=C]', 35: '[=O]', 36: '[=N]', 37: '[Cl]', 38: '[#C]', 39: '[Br]', 40: '[=S]', 41: '[=P]', 42: '[#N]', 43: '[C]', 44: '[N]', 45: '[O]', 46: '[S]', 47: '[P]', 48: '[F]'}\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/zinc15_selfies_tokens.txt\", \"r\") as file:\n",
    "    token_in_dataset = [line.strip() for line in file]\n",
    "\n",
    "print(\"Loaded tokens:\", token_in_dataset)\n",
    "word2index = {\"<pad>\": 0, \"<unk>\": 1, \"<sos>\": 2, \"<eos>\": 3}\n",
    "index2word = {0: \"<pad>\", 1: \"<unk>\", 2: \"<sos>\", 3: \"<eos>\"}\n",
    "\n",
    "start_index = max(index2word.keys()) + 1\n",
    "\n",
    "for i, token in enumerate(token_in_dataset, start=start_index):\n",
    "    word2index[token] = i\n",
    "    index2word[i] = token\n",
    "\n",
    "print(\"word2index:\", word2index)\n",
    "print(\"index2word:\", index2word)\n",
    "\n",
    "data_path = './data/zinc15_sample.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "smiles = df['smiles'].to_numpy()\n",
    "selfies = df['selfies'].to_numpy()\n",
    "\n",
    "GPU_NUM = 0\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "dataset = selfiesDataset(selfies, None, word2index, device, num_samples=None)\n",
    "data_loader = DataLoader(dataset,\n",
    "                     batch_size=128,\n",
    "                     shuffle=True,\n",
    "                     collate_fn=lambda x: collate_fn_pre(x, word2index, dataset.pattern, device))\n",
    "data_len = len(dataset)\n",
    "print(data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "375d2f42-4028-4278-a629-8a8c05b375b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLWeightScheduler:\n",
    "    def __init__(self, initial_weight=0.0, final_weight=10.0, total_epochs=100, midpoint=None):\n",
    "        self.initial_weight = initial_weight\n",
    "        self.final_weight = final_weight\n",
    "        self.total_epochs = total_epochs\n",
    "        self.midpoint = midpoint if midpoint is not None else total_epochs / 2\n",
    "\n",
    "    def get_weight(self, current_epoch):\n",
    "        # Sigmoid function parameters\n",
    "        growth_rate = 10 / self.total_epochs  # Adjust growth rate as needed\n",
    "        \n",
    "        # Sigmoid function to calculate KL weight\n",
    "        weight = self.initial_weight + (self.final_weight - self.initial_weight) / (1 + np.exp(-growth_rate * (current_epoch - self.midpoint)))\n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1559c1c2-c397-4c74-b50a-3720b72df742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from tqdm.auto import trange\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "model_name = 'pretrained_vae_zinc15'\n",
    "save_path = f'./model/{model_name}/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "scheduler = KLWeightScheduler(initial_weight = 0.0,\n",
    "                                   final_weight=1.0,\n",
    "                                   total_epochs=num_epoch,\n",
    "                                   midpoint=15)\n",
    "weights = [kl_w_scheduler.get_weight(epoch) for epoch in range(num_epoch)]\n",
    "\n",
    "embed_dim = 256                   #Embedding Vector Dim \n",
    "hidden_dim = 512                   #Latent Vector Dim\n",
    "latent_dim = 256\n",
    "en_n_l = 3                         #Encoder GRU Number of Layers \n",
    "de_n_l = 2                         #Decoder GRU Number of Layers\n",
    "base_batch_size = 128                   # Batch Size of training data\n",
    "learning_rate = 1e-4\n",
    "\n",
    "from VAE import *\n",
    "model = VAE(voca_dim=len(word2index),\n",
    "            embed_dim=embed_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            latent_dim=latent_dim,\n",
    "            en_num_layers=en_n_l,\n",
    "            de_num_layers=de_n_l,\n",
    "            prop_num = prop_num,\n",
    "            run_predictor = False, #because pre-training\n",
    "            value_range = None).to(device)\n",
    "\n",
    "NLL = nn.NLLLoss(reduction='none',\n",
    "                 ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c4c11-c6cc-492c-ab62-4aaa1120bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_log = []\n",
    "iterate_log = []\n",
    "loss_log = []\n",
    "rec_loss_log = []\n",
    "kl_loss_log = []\n",
    "ori_kl_loss_log = []\n",
    "\n",
    "\n",
    "\n",
    "total_loss = 0\n",
    "is_print = True\n",
    "\n",
    "#Teacher Forcing Annealing\n",
    "initial_tf_ratio = 1.0\n",
    "final_tf_ratio = 0.0\n",
    "tf_decay_rate = (initial_tf_ratio - final_tf_ratio) / num_epoch\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "non_zero = 1e-8  # add this when calculate log() e.g. log(std + non_zero)\n",
    "i = -1    \n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "for epoch in trange(start_epoch, num_epoch, desc=\"Epochs\", leave=False): \n",
    "    kl_w_rate = scheduler.get_weight(current_epoch = epoch)\n",
    "    tf_ratio = max(initial_tf_ratio - (epoch * tf_decay_rate), final_tf_ratio)\n",
    "    \n",
    "    for batch in tqdm(data_loader, desc=f\"Dataset({data_len})/batch({base_batch_size})\", leave=False):\n",
    "        \n",
    "        \n",
    "        model.train()\n",
    "        i += 1\n",
    "        one_time = time.time()\n",
    "        sorted_source, sorted_target, sorted_lengths, max_len, sorted_origin_indexs = batch\n",
    "        batch_size = len(sorted_source)\n",
    "        src = sorted_source.to(device)# ['<sos>', O', '=', 'C'] \n",
    "        trg = sorted_target.to(device)# ['O', '=', 'C', '<eos>'] \n",
    "\n",
    "        x, y, z, sample_z = model.forward(src, trg, sorted_lengths, max_len, tf_ratio = tf_ratio)\n",
    "        x_ = x.view(-1, x.shape[2]).contiguous()\n",
    "        x_label = sorted_target.view(-1).contiguous().to(device) \n",
    "\n",
    "       # Reconstruct Loss\n",
    "        rec_loss = NLL(x_, x_label)  # rec_loss = F.binary_cross_entropy(x,  )\n",
    "        rec_loss = rec_loss.view(x.shape[0], x.shape[1])\n",
    "        rec_loss = torch.sum(rec_loss, dim=-1)  #[batch, token_num ] -> [batch]\n",
    "        rec_loss = torch.sum(rec_loss) / batch_size\n",
    "    \n",
    "        # KLD Loss \n",
    "        \n",
    "        mu, std = z\n",
    "        kl_loss = torch.mean(mu ** 2 + std ** 2 - 2 * torch.log(std + non_zero) - 1, dim=1)\n",
    "        ori_kl_loss = torch.mean(kl_loss).item()\n",
    "        kl_loss = kl_w_rate * torch.mean(kl_loss)\n",
    "        loss = rec_loss + kl_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        # Log \n",
    "        epoch_log.append(epoch)\n",
    "        iterate_log.append(i)\n",
    "        loss_log.append(loss.item())\n",
    "        rec_loss_log.append(rec_loss.item())\n",
    "        kl_loss_log.append(kl_loss.item())\n",
    "        ori_kl_loss_log.append(ori_kl_loss)\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            if is_print:\n",
    "                print(f'=========================epoch : {epoch}==========================')\n",
    "                print(f'# Model Name : {model_name}')\n",
    "                print(\"Iteration \", i, \", Total loss \", loss.item(), \"\\nKL loss \", kl_loss.item(),\n",
    "                      \", Rec loss \", rec_loss.item(), sep=\"\")\n",
    "                print(\"Origin_KL_loss : \", ori_kl_loss)\n",
    "                print(\"Time: \", convert_time(time.time() - one_time))\n",
    "                print(\"Teacher Forcing Ratio : \", tf_ratio)\n",
    "                print(\"KLD weight : \", kl_w_rate)\n",
    "                idx = print_token2sf(sorted_target, x, word2index, index2word, batch_size, num=1)\n",
    "                idx = idx[0].item()\n",
    "                print(\" idx : \",idx)\n",
    "                print(\"-----\")\n",
    "                _, tmp_a = torch.max(x[idx], dim=-1)\n",
    "                tmp_a = tmp_a.reshape(-1).cpu().detach().numpy()\n",
    "                in_sentence = [index2word[i] for i in tmp_a]\n",
    "                in_smi = ''.join(in_sentence)\n",
    "                print(in_smi)\n",
    "                print(\"===============================================================\")\n",
    "            one_time = time.time()\n",
    "        torch.save({'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    }, f'{save_path}{model_name}_main.pt')\n",
    "        df = pd.DataFrame({'epoch': epoch_log,\n",
    "                           'i': iterate_log,\n",
    "                           'loss': loss_log,\n",
    "                           'rec_loss': rec_loss_log,\n",
    "                           'kl_loss': kl_loss_log,\n",
    "                           'ori_kl_loss' : ori_kl_loss_log,})\n",
    "        df.to_csv(f'{save_path}Log_{model_name}.csv',\n",
    "                  index=False)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'{save_path}{model_name}_epoch_{epoch}.pt')\n",
    "\n",
    "mean_iteration_loss = total_loss / i\n",
    "print(\"===============================================================\")\n",
    "print(\"Completed Epoch\", epoch, \", Total loss Mean: \", mean_iteration_loss, \", Time: \",\n",
    "      convert_time(time.time() - start_time))\n",
    "print(\"===============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ed7d4-f30d-4830-800c-31a5a1e54340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb481c-efea-4187-b7a6-ea8e3245a30f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (denovo_vae)",
   "language": "python",
   "name": "denovo_vae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
