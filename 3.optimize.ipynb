{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d14fd52-a447-48aa-9a35-88bdd40e594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm \n",
    "from rdkit.Chem import QED\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import itertools\n",
    "import importlib\n",
    "import random\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63eaa19c-867f-4cfd-ba1e-919a19f7b697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokens: ['[=Branch1]', '[#Branch1]', '[=Branch2]', '[#Branch2]', '[Branch1]', '[Branch2]', '[=Ring1]', '[=Ring2]', '[Ring1]', '[Ring2]', '[NH1+1]', '[CH1-1]', '[=N+1]', '[=N-1]', '[=S+1]', '[=PH1]', '[#N+1]', '[N+1]', '[O-1]', '[NH1]', '[CH0]', '[N-1]', '[OH0]', '[PH1]', '[C-1]', '[S+1]', '[CH1]', '[NH0]', '[PH0]', '[SH1]', '[=C]', '[=O]', '[=N]', '[Cl]', '[#C]', '[Br]', '[=S]', '[=P]', '[#N]', '[C]', '[N]', '[O]', '[S]', '[P]', '[F]']\n",
      "word2index: {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3, '[=Branch1]': 4, '[#Branch1]': 5, '[=Branch2]': 6, '[#Branch2]': 7, '[Branch1]': 8, '[Branch2]': 9, '[=Ring1]': 10, '[=Ring2]': 11, '[Ring1]': 12, '[Ring2]': 13, '[NH1+1]': 14, '[CH1-1]': 15, '[=N+1]': 16, '[=N-1]': 17, '[=S+1]': 18, '[=PH1]': 19, '[#N+1]': 20, '[N+1]': 21, '[O-1]': 22, '[NH1]': 23, '[CH0]': 24, '[N-1]': 25, '[OH0]': 26, '[PH1]': 27, '[C-1]': 28, '[S+1]': 29, '[CH1]': 30, '[NH0]': 31, '[PH0]': 32, '[SH1]': 33, '[=C]': 34, '[=O]': 35, '[=N]': 36, '[Cl]': 37, '[#C]': 38, '[Br]': 39, '[=S]': 40, '[=P]': 41, '[#N]': 42, '[C]': 43, '[N]': 44, '[O]': 45, '[S]': 46, '[P]': 47, '[F]': 48}\n",
      "index2word: {0: '<pad>', 1: '<unk>', 2: '<sos>', 3: '<eos>', 4: '[=Branch1]', 5: '[#Branch1]', 6: '[=Branch2]', 7: '[#Branch2]', 8: '[Branch1]', 9: '[Branch2]', 10: '[=Ring1]', 11: '[=Ring2]', 12: '[Ring1]', 13: '[Ring2]', 14: '[NH1+1]', 15: '[CH1-1]', 16: '[=N+1]', 17: '[=N-1]', 18: '[=S+1]', 19: '[=PH1]', 20: '[#N+1]', 21: '[N+1]', 22: '[O-1]', 23: '[NH1]', 24: '[CH0]', 25: '[N-1]', 26: '[OH0]', 27: '[PH1]', 28: '[C-1]', 29: '[S+1]', 30: '[CH1]', 31: '[NH0]', 32: '[PH0]', 33: '[SH1]', 34: '[=C]', 35: '[=O]', 36: '[=N]', 37: '[Cl]', 38: '[#C]', 39: '[Br]', 40: '[=S]', 41: '[=P]', 42: '[#N]', 43: '[C]', 44: '[N]', 45: '[O]', 46: '[S]', 47: '[P]', 48: '[F]'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (embed): Embedding(49, 256)\n",
       "  (generator): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=49, bias=False)\n",
       "    (1): LogSoftmax(dim=-1)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (gru): GRU(256, 512, num_layers=3, batch_first=True, bidirectional=True)\n",
       "    (mu): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (std): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (1): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (bridge): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (decoder_gru): GRU(256, 512, num_layers=2, batch_first=True)\n",
       "    (pre_output): Linear(in_features=768, out_features=512, bias=True)\n",
       "  )\n",
       "  (predictors): ModuleList(\n",
       "    (0): Predictor(\n",
       "      (latent_layer_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (latent_layer_2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (latent_layer_3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (latent_layer_4): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "    (1): Predictor(\n",
       "      (latent_layer_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (latent_layer_2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (latent_layer_3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (latent_layer_4): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "    (2): Predictor(\n",
       "      (latent_layer_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (latent_layer_2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (latent_layer_3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (latent_layer_4): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/zinc15_selfies_tokens.txt\", \"r\") as file:\n",
    "    token_in_dataset = [line.strip() for line in file]\n",
    "\n",
    "print(\"Loaded tokens:\", token_in_dataset)\n",
    "word2index = {\"<pad>\": 0, \"<unk>\": 1, \"<sos>\": 2, \"<eos>\": 3}\n",
    "index2word = {0: \"<pad>\", 1: \"<unk>\", 2: \"<sos>\", 3: \"<eos>\"}\n",
    "\n",
    "start_index = max(index2word.keys()) + 1\n",
    "\n",
    "for i, token in enumerate(token_in_dataset, start=start_index):\n",
    "    word2index[token] = i\n",
    "    index2word[i] = token\n",
    "    \n",
    "print(\"word2index:\", word2index)\n",
    "print(\"index2word:\", index2word)\n",
    "\n",
    "data_path = './data/zinc15_sample_test.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "smiles = df['smiles'].to_numpy()\n",
    "selfies = df['selfies'].to_numpy()\n",
    "\n",
    "prop_num = 3\n",
    "prop_data = [df['bcl2'].to_numpy(),\n",
    "             df['bclxl'].to_numpy(),\n",
    "             df['bclw'].to_numpy()]\n",
    "\n",
    "prop_name = ['bcl2', 'bclxl', 'bclw']\n",
    "\n",
    "embed_dim = 256                   #Embedding Vector Dim \n",
    "hidden_dim = 512                   #Latent Vector Dim\n",
    "latent_dim = 256\n",
    "en_n_l = 3                         #Encoder GRU Number of Layers \n",
    "de_n_l = 2                         #Decoder GRU Number of Layers\n",
    "base_batch_size = 128                   # Batch Size of training data\n",
    "learning_rate = 1e-4\n",
    "\n",
    "GPU_NUM = 0\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "\n",
    "from VAE import *\n",
    "model = VAE(voca_dim=len(word2index),\n",
    "            embed_dim=embed_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            latent_dim=latent_dim,\n",
    "            en_num_layers=en_n_l,\n",
    "            de_num_layers=de_n_l,\n",
    "            prop_num = prop_num,\n",
    "            run_predictor = True,\n",
    "            value_range = None).to(device)\n",
    "pt_path = f'./model/finetuned_Model_A.pt'\n",
    "state_dict = torch.load(pt_path)['model_state_dict']\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a14db55f-0527-4059-a0ff-104e208ea6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                smiles      bcl2     bclxl  \\\n",
      "267  CN(CC(=O)NC(C)(C)C)C(=O)C1CCN(c2ccc(S(=O)(=O)N...  5.038860  4.875372   \n",
      "987  O=C(NCCc1ccccc1)C1CN(C(=O)CN(C2CCCCC2)S(=O)(=O...  5.755924  5.237386   \n",
      "874  Cc1sc2ncnc(N3CCC(C(=O)N4CCN(S(=O)(=O)c5ccc(C(C...  5.225762  4.832924   \n",
      "\n",
      "         bclw  \n",
      "267  5.214093  \n",
      "987  6.090002  \n",
      "874  5.098664  \n"
     ]
    }
   ],
   "source": [
    "target_score_dict = {'qed' : (0.8, 'up', {'min' : 0, 'max' : 1}),\n",
    "                     'SAs' : (3.0, 'down', {'min' : 1, 'max' : 10}), \n",
    "                     'bcl2' : (10.34, 'up', {'min' : 2.15, 'max' : 10.34}),\n",
    "                     'bclxl' : (9.85, 'up', {'min' : 1.91, 'max' : 9.85}),\n",
    "                     'bclw' : (8.68, 'up', {'min' : 2.23, 'max' : 8.68})}\n",
    "target_list = [ target_score_dict[str][0] for str in prop_name] \n",
    "target_up = [ True if target_score_dict[str][1] == 'up' else False for str in prop_name]  #Target up\n",
    "target_tensor = torch.tensor(target_list).to(device)\n",
    "target_max = [ target_score_dict[str][2]['max'] for str in prop_name ]\n",
    "target_min = [ target_score_dict[str][2]['min'] for str in prop_name ]\n",
    "\n",
    "\n",
    "\n",
    "final_df = df.sample(n=100, random_state=2025)\n",
    "smiles = final_df['smiles'].to_numpy()\n",
    "selfies = final_df['selfies'].to_numpy()\n",
    "\n",
    "prop_data = []\n",
    "for i in range(prop_num):\n",
    "    prop_data.append(final_df[prop_name[i]].to_numpy())\n",
    "\n",
    "dataset = selfiesDataset(selfies, prop_data, word2index, device, num_samples=None)\n",
    "data_loader = DataLoader(dataset,\n",
    "                         batch_size=1,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=lambda x: collate_fn(x, word2index, dataset.pattern, device))\n",
    "print(final_df[['smiles']+ prop_name][0:3])\n",
    "\n",
    "#1. define odds weight  by importance sampling \n",
    "tau = 0.4\n",
    "\n",
    "#Get MPL prop\n",
    "_, odds_full = model.mask_inference(thold=0.0)   # odds_full: (prop_num, latent_dim, 2)\n",
    "odds = odds_full[:, :, 1]   \n",
    "\n",
    "#odds_ratio \n",
    "odds_scaled = odds ** (1.0 / tau)\n",
    "\n",
    "weights = odds_scaled / (odds_scaled.mean(dim=1, keepdim=True) + 1e-8)\n",
    "weights += 1.0   \n",
    "weight_vectors = [weights[i].clone() for i in range(weights.size(0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d915fa19-3aaa-468b-81b2-c2f27a89cd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= =step : 4, seed 0 Optimize Results === = = =\n",
      "Current Step Z mean: -0.06611023098230362\n",
      "####################\n",
      "smi :  CN(CC(=O)NC(C)(C)C)C(=O)C1CCN(c2ccc(S(=O)(=O)N3CCCCC3)cc2[N+](=O)[O-])CC1\n",
      "Target Loss :  272.7717876434326\n",
      "####################\n",
      "# Property Info 0  - bcl2 -  # \n",
      "Property Range | 2.15(min) ~ 10.34(max) |\n",
      "Target : 10.34\n",
      "Predict : 5.366420269012451\n",
      "####################\n",
      "# Property Info 1  - bclxl -  # \n",
      "Property Range | 1.91(min) ~ 9.85(max) |\n",
      "Target : 9.85\n",
      "Predict : 5.741135120391846\n",
      "####################\n",
      "# Property Info 2  - bclw -  # \n",
      "Property Range | 2.23(min) ~ 8.68(max) |\n",
      "Target : 8.68\n",
      "Predict : 5.083462238311768\n",
      "= = = === = = = === = = = === = = = === = = = === = = =\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m ori_t_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     66\u001b[0m pred_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 68\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_z\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m154\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msos_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msos_tensor\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     69\u001b[0m smi_src, sf \u001b[38;5;241m=\u001b[39m tensor2smiles_sampling(x, word2index, index2word)  \n\u001b[1;32m     70\u001b[0m smi \u001b[38;5;241m=\u001b[39m normalize_SMILES(smi_src[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/cgw_VAE/github/VAE.py:343\u001b[0m, in \u001b[0;36mVAE.decode_z\u001b[0;34m(self, z, max_len, sos_tensor, return_y)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_z\u001b[39m(\u001b[38;5;28mself\u001b[39m, z, max_len, sos_tensor, return_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    342\u001b[0m     sos_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(sos_tensor)\n\u001b[0;32m--> 343\u001b[0m     reconstruct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43membeder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43msos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msos_token\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(reconstruct)\n\u001b[1;32m    349\u001b[0m     latent_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_inference()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# [1, prop_num, latent_dim]\u001b[39;00m\n",
      "File \u001b[0;32m~/cgw_VAE/github/VAE.py:181\u001b[0m, in \u001b[0;36mDecoder.generate\u001b[0;34m(self, z, max_len, generator, embeder, sos_token)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# [batch_size, 256] \u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len): \n\u001b[0;32m--> 181\u001b[0m     pre_output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     pre_output_vectors\u001b[38;5;241m.\u001b[39mappend(pre_output)\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mis\u001b[39;00m max_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/cgw_VAE/github/VAE.py:164\u001b[0m, in \u001b[0;36mDecoder.forward_step\u001b[0;34m(self, prev_embed, hidden)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, prev_embed, hidden):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m#output -> [batch, 1, hidden_dim]\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m#hidden -> [num_layer, batch, hidden_dim]\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_gru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# input-feeding approach\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     pre_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([prev_embed, output], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# [batch, 1, embed_dim + hidden_dim]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:955\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    959\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#Log 저장 경로\n",
    "model_name = f\"Model_A_Optim\"\n",
    "save_optim_path = f'./model/{model_name}/'\n",
    "\n",
    "parent_num = 10\n",
    "stop_condi = 3000\n",
    "\n",
    "t_weight = 15\n",
    "prop_weight = [1, 1, 1]\n",
    "\n",
    "MSE = torch.nn.MSELoss(reduction=\"sum\")\n",
    "GaussianNLL = torch.nn.GaussianNLLLoss(reduction=\"sum\")\n",
    "\n",
    "optim_step = 50\n",
    "optim_iter = 20  \n",
    "\n",
    "y_list = []\n",
    "for p_i, batch in enumerate(data_loader):\n",
    "    # Define Model\n",
    "    \n",
    "    import VAE as vae\n",
    "    importlib.reload(vae)\n",
    "    from VAE import *\n",
    "    model = VAE(voca_dim=len(word2index),\n",
    "                embed_dim=embed_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                latent_dim=latent_dim,\n",
    "                en_num_layers=en_n_l,\n",
    "                de_num_layers=de_n_l,\n",
    "                prop_num = prop_num,\n",
    "                run_predictor = True,\n",
    "                value_range = None).to(device)\n",
    "    pt_path = f'./model/finetuned_Model_A.pt'\n",
    "    state_dict = torch.load(pt_path)['model_state_dict']\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    #\\eta parameter\n",
    "    z_param = nn.Parameter(torch.randn(1, latent_dim), requires_grad=True)\n",
    "\n",
    "    sorted_source, sorted_target, sorted_lengths, max_len, sorted_props, sorted_origin_indexs = batch\n",
    "    batch_size = len(sorted_source)\n",
    "    src = sorted_source.to(device)\n",
    "    trg = sorted_target.to(device)\n",
    "    x, y, _, sample_z, _ = model.inference(src,\n",
    "                                           sorted_lengths,\n",
    "                                           max_len)\n",
    "\n",
    "    z_param.data = sample_z.clone().data  \n",
    "    z_param.requires_grad = True\n",
    "\n",
    "    optimizer = optim.Adam([z_param], lr=0.01)\n",
    "    sos_tensor = torch.tensor([word2index['<sos>']]).to(device)\n",
    "    smi_src, sf = tensor2smiles_v2(x, word2index, index2word)\n",
    "    smi = smi_src[0]\n",
    "\n",
    "    _, y = model.decode_z(z_param, max_len = 154, sos_tensor=sos_tensor, return_y = True) \n",
    "    y_values = [tensor_item.cpu().detach().numpy().flatten()[0] for tensor_item in y]\n",
    "    y_list.append(y_values)\n",
    "    \n",
    "    for gen_try in range(optim_step * optim_iter):\n",
    "        individual_losses = []\n",
    "        ori_t_losses = []\n",
    "        pred_list = []\n",
    "        \n",
    "        x = model.decode_z(z_param, max_len = 154, sos_tensor=sos_tensor) \n",
    "        smi_src, sf = tensor2smiles_sampling(x, word2index, index2word)  \n",
    "        smi = normalize_SMILES(smi_src[0])\n",
    "        optimizer.zero_grad() \n",
    "        total_grad_z = torch.zeros_like(z_param) \n",
    "\n",
    "        for idx, (predictor, target, is_up, p_max, p_min) in enumerate(zip(model.predictors, target_tensor, target_up, target_max, target_min)):\n",
    "            pred_y = predictor(z_param)\n",
    "            pred_list.append(pred_y)\n",
    "            t_loss = MSE(pred_y.squeeze(0).view(target.shape), target)\n",
    "            individual_losses.append(prop_weight[idx] * t_loss)\n",
    "            ori_t_losses.append(t_loss)\n",
    "    \n",
    "            weight_vector = weight_vectors[idx]\n",
    "            weight_vector = weight_vector.to(dtype=z_param.dtype, device=z_param.device)\n",
    "    \n",
    "            grad_z = torch.autograd.grad(prop_weight[idx] * t_loss, z_param, retain_graph=True)[0]\n",
    "    \n",
    "            weighted_grad_z = grad_z * weight_vector.unsqueeze(0)\n",
    "    \n",
    "            total_grad_z += weighted_grad_z\n",
    "\n",
    "        target_loss = torch.mean(torch.stack(individual_losses))\n",
    "        loss = (t_weight * target_loss)\n",
    "        z_param.grad = total_grad_z\n",
    "        optimizer.step()        \n",
    "        #loss.backward()\n",
    "\n",
    "        if (gen_try % optim_iter == 0):         \n",
    "            _, y = model.decode_z(z_param, max_len = 154, sos_tensor=sos_tensor, return_y = True) \n",
    "            y_values = [tensor_item.cpu().detach().numpy().flatten()[0] for tensor_item in y]\n",
    "            y_list.append(y_values)\n",
    "\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            print(f'= =step : {int(gen_try / optim_iter)}, seed {p_i} Optimize Results === = = =')\n",
    "            print(\"Current Step Z mean:\", z_param.data.mean().item())\n",
    "            print(\"#\" * 20)\n",
    "            print(\"smi : \", smi)\n",
    "            print(\"Target Loss : \", t_weight * target_loss.item())\n",
    "\n",
    "            for i in range(prop_num):\n",
    "                print('#'* 20 )\n",
    "                print(f'# Property Info {i}  - {prop_name[i]} -  # ')\n",
    "                print(f'Property Range | {target_min[i]}(min) ~ {target_max[i]}(max) |')\n",
    "                print(f'Target : {target_list[i]}')\n",
    "                print(f'Predict : {pred_list[i].item()}')\n",
    "\n",
    "            print(f'= = = === = = = === = = = === = = = === = = = === = = =')\n",
    "    mask_y_list = list(map(list, zip(*y_list)))\n",
    "\n",
    "print(\"End of Optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce8e48-7e28-4442-8cd5-cec0410002be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (denovo_vae)",
   "language": "python",
   "name": "denovo_vae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
