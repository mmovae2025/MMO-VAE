{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd359e7-4515-4777-a568-a10809eb2e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm \n",
    "from rdkit.Chem import QED\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import itertools\n",
    "import importlib\n",
    "import random\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eedfcae-745f-4400-a841-98d65245cd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokens: ['[=Branch1]', '[#Branch1]', '[=Branch2]', '[#Branch2]', '[Branch1]', '[Branch2]', '[=Ring1]', '[=Ring2]', '[Ring1]', '[Ring2]', '[NH1+1]', '[CH1-1]', '[=N+1]', '[=N-1]', '[=S+1]', '[=PH1]', '[#N+1]', '[N+1]', '[O-1]', '[NH1]', '[CH0]', '[N-1]', '[OH0]', '[PH1]', '[C-1]', '[S+1]', '[CH1]', '[NH0]', '[PH0]', '[SH1]', '[=C]', '[=O]', '[=N]', '[Cl]', '[#C]', '[Br]', '[=S]', '[=P]', '[#N]', '[C]', '[N]', '[O]', '[S]', '[P]', '[F]']\n",
      "word2index: {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3, '[=Branch1]': 4, '[#Branch1]': 5, '[=Branch2]': 6, '[#Branch2]': 7, '[Branch1]': 8, '[Branch2]': 9, '[=Ring1]': 10, '[=Ring2]': 11, '[Ring1]': 12, '[Ring2]': 13, '[NH1+1]': 14, '[CH1-1]': 15, '[=N+1]': 16, '[=N-1]': 17, '[=S+1]': 18, '[=PH1]': 19, '[#N+1]': 20, '[N+1]': 21, '[O-1]': 22, '[NH1]': 23, '[CH0]': 24, '[N-1]': 25, '[OH0]': 26, '[PH1]': 27, '[C-1]': 28, '[S+1]': 29, '[CH1]': 30, '[NH0]': 31, '[PH0]': 32, '[SH1]': 33, '[=C]': 34, '[=O]': 35, '[=N]': 36, '[Cl]': 37, '[#C]': 38, '[Br]': 39, '[=S]': 40, '[=P]': 41, '[#N]': 42, '[C]': 43, '[N]': 44, '[O]': 45, '[S]': 46, '[P]': 47, '[F]': 48}\n",
      "index2word: {0: '<pad>', 1: '<unk>', 2: '<sos>', 3: '<eos>', 4: '[=Branch1]', 5: '[#Branch1]', 6: '[=Branch2]', 7: '[#Branch2]', 8: '[Branch1]', 9: '[Branch2]', 10: '[=Ring1]', 11: '[=Ring2]', 12: '[Ring1]', 13: '[Ring2]', 14: '[NH1+1]', 15: '[CH1-1]', 16: '[=N+1]', 17: '[=N-1]', 18: '[=S+1]', 19: '[=PH1]', 20: '[#N+1]', 21: '[N+1]', 22: '[O-1]', 23: '[NH1]', 24: '[CH0]', 25: '[N-1]', 26: '[OH0]', 27: '[PH1]', 28: '[C-1]', 29: '[S+1]', 30: '[CH1]', 31: '[NH0]', 32: '[PH0]', 33: '[SH1]', 34: '[=C]', 35: '[=O]', 36: '[=N]', 37: '[Cl]', 38: '[#C]', 39: '[Br]', 40: '[=S]', 41: '[=P]', 42: '[#N]', 43: '[C]', 44: '[N]', 45: '[O]', 46: '[S]', 47: '[P]', 48: '[F]'}\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/zinc15_selfies_tokens.txt\", \"r\") as file:\n",
    "    token_in_dataset = [line.strip() for line in file]\n",
    "\n",
    "print(\"Loaded tokens:\", token_in_dataset)\n",
    "\n",
    "word2index = {\"<pad>\": 0, \"<unk>\": 1, \"<sos>\": 2, \"<eos>\": 3}\n",
    "index2word = {0: \"<pad>\", 1: \"<unk>\", 2: \"<sos>\", 3: \"<eos>\"}\n",
    "\n",
    "start_index = max(index2word.keys()) + 1\n",
    "\n",
    "for i, token in enumerate(token_in_dataset, start=start_index):\n",
    "    word2index[token] = i\n",
    "    index2word[i] = token\n",
    "\n",
    "print(\"word2index:\", word2index)\n",
    "print(\"index2word:\", index2word)\n",
    "\n",
    "data_path = './data/zinc15_sample.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "smiles = df['smiles'].to_numpy()\n",
    "selfies = df['selfies'].to_numpy()\n",
    "\n",
    "prop_num = 3\n",
    "prop_data = [df['bcl2'].to_numpy(),\n",
    "             df['bclxl'].to_numpy(),\n",
    "             df['bclw'].to_numpy()]\n",
    "prop_name = ['bcl2', 'bclxl', 'bclw']\n",
    "\n",
    "GPU_NUM = 0\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "dataset = selfiesDataset(selfies, prop_data, word2index, device, num_samples=None)\n",
    "data_loader = DataLoader(dataset,\n",
    "                     batch_size=128,\n",
    "                     shuffle=True,\n",
    "                     collate_fn=lambda x: collate_fn(x, word2index, dataset.pattern, device))\n",
    "data_len = len(dataset)\n",
    "print(data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3acb04bd-b571-4a76-baa6-8355b7994b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from tqdm.auto import trange\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "model_name = 'finetuned_vae_zinc15'\n",
    "save_path = f'./model/{model_name}/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "from VAE import *\n",
    "\n",
    "embed_dim = 256                   #Embedding Vector Dim \n",
    "hidden_dim = 512                   #Latent Vector Dim\n",
    "latent_dim = 256\n",
    "en_n_l = 3                         #Encoder GRU Number of Layers \n",
    "de_n_l = 2                         #Decoder GRU Number of Layers\n",
    "base_batch_size = 128                   # Batch Size of training data\n",
    "learning_rate = 1e-4\n",
    "\n",
    "from VAE import *\n",
    "model = VAE(voca_dim=len(word2index),\n",
    "            embed_dim=embed_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            latent_dim=latent_dim,\n",
    "            en_num_layers=en_n_l,\n",
    "            de_num_layers=de_n_l,\n",
    "            prop_num = prop_num,\n",
    "            run_predictor = True,\n",
    "            value_range = None).to(device)\n",
    "\n",
    "pt_path = f'./model/pretrained_vae_zinc15.pt'\n",
    "state_dict = torch.load(pt_path)['model_state_dict']\n",
    "filtered_state_dict = {k: v for k, v in state_dict.items() if not (k.startswith('predictors') or k.startswith('latent_mask'))}\n",
    "model.load_state_dict(filtered_state_dict, strict=False)\n",
    "\n",
    "\n",
    "NLL = nn.NLLLoss(reduction='none',\n",
    "                 ignore_index=0)  # Reconstruct Loss \n",
    "GaussianNLL = torch.nn.GaussianNLLLoss(reduction=\"sum\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e6208ea-daa8-4637-8624-da68ebf591ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_log = []\n",
    "iterate_log = []\n",
    "loss_log = []\n",
    "rec_loss_log = []\n",
    "kl_loss_log = []\n",
    "prop_loss_log = []\n",
    "ori_kl_loss_log = []\n",
    "ori_prop_loss_log = []\n",
    "\n",
    "total_loss = 0\n",
    "is_print = True\n",
    "\n",
    "kl_w_rate = 0.1\n",
    "prop_weight = [10, 10, 10] # Model A is Same Weight  \n",
    "sos_tensor = torch.tensor([word2index['<sos>']]).to(device)\n",
    "start_time = time.time()\n",
    "non_zero = 1e-8  # add this when calculate log() e.g. log(std + non_zero)\n",
    "i = -1    \n",
    "\n",
    "start_epoch = 0\n",
    "num_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb885356-84c3-4f1f-b697-484da4327d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in trange(start_epoch, num_epoch, desc=\"Epcohs\", leave=False):    \n",
    "    for batch in tqdm(data_loader, desc=f\"Dataset({data_len})/batch({base_batch_size})\", leave=False):\n",
    "        model.train()\n",
    "        i += 1\n",
    "        one_time = time.time()\n",
    "        sorted_source, sorted_target, sorted_lengths, max_len, sorted_props, sorted_origin_indexs = batch\n",
    "        batch_size = len(sorted_source)\n",
    "        src = sorted_source.to(device)# ['<sos>', O', '=', 'C']\n",
    "        trg = sorted_target.to(device)# ['O', '=', 'C', '<eos>']\n",
    "    \n",
    "    \n",
    "        tf_ratio = 0.0\n",
    "        x, y, z, sample_z, mask = model.forward(src, trg, sorted_lengths, max_len, tf_ratio = tf_ratio)\n",
    "        x_ = x.view(-1, x.shape[2]).contiguous()\n",
    "        x_label = sorted_target.view(-1).contiguous().to(device) \n",
    "        \n",
    "        # Reconstruct Loss\n",
    "        rec_loss = NLL(x_, x_label)  # rec_loss = F.binary_cross_entropy(x,  )\n",
    "        rec_loss = rec_loss.view(x.shape[0], x.shape[1])\n",
    "        rec_loss = torch.sum(rec_loss, dim=-1) # [batch, token_num ] -> [batch]\n",
    "        rec_loss = torch.sum(rec_loss) / batch_size\n",
    "    \n",
    "        # KLD Loss \n",
    "        \n",
    "        mu, std = z\n",
    "        kl_loss = torch.mean(mu ** 2 + std ** 2 - 2 * torch.log(std + non_zero) - 1, dim=1)\n",
    "        ori_kl_loss = torch.mean(kl_loss).item()\n",
    "        kl_loss = kl_w_rate * torch.mean(kl_loss)\n",
    "    \n",
    "        prop_loss = []\n",
    "        props_to_iterate = sorted_props\n",
    "        for idx, y_label in enumerate(props_to_iterate):\n",
    "            y_label_float = torch.tensor(y_label, dtype=torch.float).unsqueeze(1).to(device)\n",
    "        \n",
    "            predicted_mean = y[idx]  \n",
    "            target = y_label_float  \n",
    "            variance = torch.ones_like(predicted_mean) \n",
    "        \n",
    "            # Gaussian NLL Loss\n",
    "            y_loss = GaussianNLL(predicted_mean, target, variance) / batch_size\n",
    "            prop_loss.append(y_loss) \n",
    "            \n",
    "        weight_prop_loss = [(prop_loss[k] * prop_weight[k]) / (prop_num) for k in range(prop_num)]\n",
    "        sum_prop_loss = sum(weight_prop_loss)\n",
    "        loss = rec_loss + kl_loss + sum_prop_loss \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        # Log \n",
    "        epoch_log.append(epoch)\n",
    "        iterate_log.append(i)\n",
    "        loss_log.append(loss.item())\n",
    "        rec_loss_log.append(rec_loss.item())\n",
    "        kl_loss_log.append(kl_loss.item())\n",
    "        ori_kl_loss_log.append(ori_kl_loss)\n",
    "        prop_loss_log.append(sum_prop_loss.item())\n",
    "        ori_prop_loss_log.append(sum(prop_loss).item())\n",
    "        if i % 100 == 0:\n",
    "            if is_print:\n",
    "                with print_out:\n",
    "                    true_false_counts = [\n",
    "                        (torch.sum(mask[0, i]).item(), (mask[0, i].numel() - torch.sum(mask[0, i])).item())\n",
    "                        for i in range(mask.size(1))\n",
    "                    ]\n",
    "                    mask_true = sum(true_count for true_count, _ in true_false_counts)\n",
    "                    mask_false = sum(false_count for _, false_count in true_false_counts)\n",
    "                    clear_output(wait=True)\n",
    "                    print(f'=========================epoch : {epoch}==========================')\n",
    "                    print(f'# Model Name : {model_name}')\n",
    "                    print(\"Iteration \", i, \", Total loss \", loss.item(), \"\\nKL loss \", kl_loss.item(),\n",
    "                          \", Rec loss \", rec_loss.item(), sep=\"\")\n",
    "                    print(\"Origin_KL_loss : \", ori_kl_loss)\n",
    "                    print(\"Origin_prop_loss : \", sum(prop_loss).item())\n",
    "                    print(f'Mask True : \" {mask_true}')\n",
    "                    print(f'Mask False : \" {mask_false}')\n",
    "                    print(f'sum(y Loss) : {sum_prop_loss.item()}')\n",
    "                    for p_i in range(prop_num):\n",
    "                        print(f'== = Property : {prop_name[p_i]} = ==')\n",
    "                        p_true, p_false = true_false_counts[p_i]\n",
    "                        print(f'Mask True Num : {p_true}')\n",
    "                        print(f'Mask False Num : {p_false}')\n",
    "                        print(f'y[{p_i}] Loss : {prop_loss[p_i].item()}')\n",
    "                        print(f'Fixed_dynamic_weight: {prop_weight[p_i]}')\n",
    "                        print(f'input_{prop_name[p_i]}[{idx}] : {sorted_props[p_i][idx]}')\n",
    "                        print(f'pred_{prop_name[p_i]}[{idx}] : {y[p_i][idx].item()}')\n",
    "                        print(\"-----------------------------\")\n",
    "    \n",
    "                    print(\"Time: \", convert_time(time.time() - one_time))\n",
    "                    print(\"Teacher Forcing Ratio : \", tf_ratio)\n",
    "                    print(\"KLD weight : \", kl_w_rate)\n",
    "                    \n",
    "                    idx = print_token2sf(sorted_target, x, word2index, index2word, batch_size, num=1)\n",
    "                    idx = idx[0].item()\n",
    "                    print(\" idx : \",idx)\n",
    "                    print(\"-----\")\n",
    "                    _, tmp_a = torch.max(x[idx], dim=-1)\n",
    "                    tmp_a = tmp_a.reshape(-1).cpu().detach().numpy()\n",
    "                    in_sentence = [index2word[i] for i in tmp_a]\n",
    "                    in_smi = ''.join(in_sentence)\n",
    "                    print(in_smi)\n",
    "                    print(\"-----\")\n",
    "                    print(\"===============================================================\")\n",
    "                one_time = time.time()\n",
    "            torch.save({'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        }, f'{save_path}{model_name}_main.pt')\n",
    "    \n",
    "            df = pd.DataFrame({'epoch': epoch_log,\n",
    "                               'i': iterate_log,\n",
    "                               'loss': loss_log,\n",
    "                               'rec_loss': rec_loss_log,\n",
    "                               'kl_loss': kl_loss_log,\n",
    "                               'prop_loss': prop_loss_log,\n",
    "                               'ori_kl_loss' : ori_kl_loss_log,\n",
    "                               'ori_prop_loss' : ori_prop_loss_log})\n",
    "            df.to_csv(f'{save_path}Log_{model_name}.csv',\n",
    "                      index=False)\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, f'{save_path}{model_name}_epoch_{epoch}.pt')\n",
    "                # Decrease tf_ratio for the next epoch \n",
    "mean_iteration_loss = total_loss / i\n",
    "print(\"===============================================================\")\n",
    "print(\"Completed Epoch\", epoch, \", Total loss Mean: \", mean_iteration_loss, \", Time: \",\n",
    "      convert_time(time.time() - start_time))\n",
    "print(\"===============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17da2f3-1de5-4f75-a7a6-6f734b434bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (denovo_vae)",
   "language": "python",
   "name": "denovo_vae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
